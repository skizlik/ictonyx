{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ictonyx Example: PyTorch Regression Variability Study\n",
    "\n",
    "This notebook trains a small network on a synthetic regression problem and reports\n",
    "the distribution of validation MSE across runs.\n",
    "\n",
    "**Requirements:** `pip install ictonyx torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import ictonyx as ix\n",
    "from ictonyx import (\n",
    "    ModelConfig,\n",
    "    PyTorchModelWrapper,\n",
    "    ArraysDataHandler,\n",
    "    run_variability_study,\n",
    ")\n",
    "\n",
    "print(f\"Ictonyx v{ix.__version__}\")\n",
    "print(f\"PyTorch v{torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We create a simple linear relationship with 5 features and Gaussian noise.\n",
    "The true weights are known, so we can verify the model is learning something real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(500, 5).astype(np.float32)\n",
    "true_weights = np.array([1.5, -2.0, 0.5, 0.0, 3.0], dtype=np.float32)\n",
    "y = X @ true_weights + rng.randn(500).astype(np.float32) * 0.3\n",
    "\n",
    "data_handler = ArraysDataHandler(X, y, test_size=0.2, val_size=0.2)\n",
    "\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"Noise level: 0.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model Builder\n",
    "\n",
    "For regression, set `task='regression'` and use an appropriate loss (MSELoss).\n",
    "The wrapper will track MSE instead of accuracy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regressor(config: ModelConfig) -> PyTorchModelWrapper:\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(5, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 1),\n",
    "    )\n",
    "    return PyTorchModelWrapper(\n",
    "        model,\n",
    "        criterion=nn.MSELoss(),\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_params={'lr': config.get('learning_rate', 0.005)},\n",
    "        task='regression',\n",
    "    )\n",
    "\n",
    "print(repr(create_regressor(ModelConfig())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Variability Study\n",
    "\n",
    "10 runs, 50 epochs each. The question: how much does final MSE vary\n",
    "across different random initializations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\n",
    "    'epochs': 50,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.005,\n",
    "    'verbose': 0,\n",
    "})\n",
    "\n",
    "results = run_variability_study(\n",
    "    model_builder=create_regressor,\n",
    "    data_handler=data_handler,\n",
    "    model_config=config,\n",
    "    num_runs=10,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summarize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available metrics:\", results.get_available_metrics())\n",
    "print()\n",
    "val_mse = results.get_metric_values('val_mse')\n",
    "print(\"Per-run val_mse:\")\n",
    "for i, mse in enumerate(val_mse, 1):\n",
    "    print(f\"  Run {i}: {mse:.4f}\")\n",
    "\n",
    "print(f\"\\nBest:  {min(val_mse):.4f}\")\n",
    "print(f\"Worst: {max(val_mse):.4f}\")\n",
    "print(f\"Range: {max(val_mse) - min(val_mse):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary DataFrame\n",
    "results.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
