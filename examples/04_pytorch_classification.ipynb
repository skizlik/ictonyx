{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ictonyx Example: PyTorch Classification Variability Study\n",
    "\n",
    "This notebook trains a simple feedforward network on the Iris dataset multiple times\n",
    "and reports the distribution of validation accuracy across runs.\n",
    "\n",
    "**Requirements:** `pip install ictonyx torch scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import ictonyx as ix\n",
    "from ictonyx import (\n",
    "    ModelConfig,\n",
    "    PyTorchModelWrapper,\n",
    "    ArraysDataHandler,\n",
    "    run_variability_study,\n",
    ")\n",
    "\n",
    "print(f\"Ictonyx v{ix.__version__}\")\n",
    "print(f\"PyTorch v{torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "We use the classic Iris dataset — 150 samples, 4 features, 3 classes.\n",
    "StandardScaler normalizes features, which helps the network converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = StandardScaler().fit_transform(iris.data).astype(np.float32)\n",
    "y = iris.target.astype(np.int64)\n",
    "\n",
    "data_handler = ArraysDataHandler(X, y, test_size=0.2, val_size=0.2)\n",
    "\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model Builder\n",
    "\n",
    "The model builder is a **factory function** that creates a fresh model for each run.\n",
    "This is essential — each variability study run needs its own randomly initialized model.\n",
    "\n",
    "The `PyTorchModelWrapper` takes:\n",
    "- An `nn.Module` (your network architecture)\n",
    "- A loss function (`criterion`)\n",
    "- An optimizer class + params (not an instance — because the optimizer must bind to each new model's parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iris_net(config: ModelConfig) -> PyTorchModelWrapper:\n",
    "    \"\"\"Factory function: creates a fresh model each run.\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(32, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 3),\n",
    "    )\n",
    "    return PyTorchModelWrapper(\n",
    "        model,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_params={'lr': config.get('learning_rate', 0.01)},\n",
    "        task='classification',\n",
    "    )\n",
    "\n",
    "# Quick sanity check\n",
    "test_wrapper = create_iris_net(ModelConfig({'learning_rate': 0.01}))\n",
    "print(repr(test_wrapper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Variability Study\n",
    "\n",
    "This trains the model 10 times, each with a different random initialization\n",
    "but deterministic seeding (seed=42), so the study is fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\n",
    "    'epochs': 30,\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 0,\n",
    "})\n",
    "\n",
    "results = run_variability_study(\n",
    "    model_builder=create_iris_net,\n",
    "    data_handler=data_handler,\n",
    "    model_config=config,\n",
    "    num_runs=10,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Examine Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.summarize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available metrics:\", results.get_available_metrics())\n",
    "print()\n",
    "print(\"Per-run val_accuracy:\")\n",
    "for i, acc in enumerate(results.get_metric_values('val_accuracy'), 1):\n",
    "    print(f\"  Run {i}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary DataFrame — one row per run, all final-epoch metrics\n",
    "results.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
