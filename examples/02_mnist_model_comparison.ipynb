{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Your New Architecture Actually Better?\n",
    "\n",
    "You've designed two CNN architectures. Model A gets 98.2% on MNIST. Model B gets 98.7%. Model B wins, right?\n",
    "\n",
    "Not so fast. You've compared single runs. That's like flipping one coin for each model and declaring a winner.\n",
    "\n",
    "In this notebook, we'll train each architecture multiple times and use proper statistics to determine if Model B is *actually* better — or just got lucky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ictonyx tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers, Input\n",
    "\n",
    "import ictonyx as ix\n",
    "from ictonyx import (\n",
    "    ModelConfig, \n",
    "    KerasModelWrapper, \n",
    "    ArraysDataHandler,\n",
    "    run_variability_study,\n",
    "    compare_two_models,\n",
    "    plot_comparison_boxplots\n",
    ")\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Ictonyx: {ix.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_train = X_train[..., np.newaxis]\n",
    "\n",
    "# Use subset for speed\n",
    "X_subset = X_train[:10000]\n",
    "y_subset = y_train[:10000]\n",
    "\n",
    "print(f\"Using {len(X_subset)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Two Competing Architectures\n",
    "\n",
    "**Model A:** A shallow CNN — two conv layers, relatively few parameters.\n",
    "\n",
    "**Model B:** A deeper CNN — three conv layers with dropout. More capacity, but also more things that can go wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_cnn(config: ModelConfig) -> KerasModelWrapper:\n",
    "    \"\"\"Model A: Simple, shallow CNN.\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return KerasModelWrapper(model, model_id='shallow_cnn')\n",
    "\n",
    "\n",
    "def create_deep_cnn(config: ModelConfig) -> KerasModelWrapper:\n",
    "    \"\"\"Model B: Deeper CNN with dropout.\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return KerasModelWrapper(model, model_id='deep_cnn')\n",
    "\n",
    "\n",
    "# Quick comparison of parameter counts\n",
    "shallow = create_shallow_cnn(ModelConfig({}))\n",
    "deep = create_deep_cnn(ModelConfig({}))\n",
    "\n",
    "print(f\"Shallow CNN: {shallow.model.count_params():,} parameters\")\n",
    "print(f\"Deep CNN: {deep.model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Variability Studies for Both Models\n",
    "\n",
    "We'll train each model 10 times and collect accuracy distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig({\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'verbose': 0\n",
    "})\n",
    "\n",
    "data_handler = ArraysDataHandler(X_subset, y_subset)\n",
    "\n",
    "print(\"Training Shallow CNN (10 runs)...\")\n",
    "shallow_results = run_variability_study(\n",
    "    model_builder=create_shallow_cnn,\n",
    "    data_handler=data_handler,\n",
    "    model_config=config,\n",
    "    num_runs=10,\n",
    "    epochs_per_run=5\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Deep CNN (10 runs)...\")\n",
    "deep_results = run_variability_study(\n",
    "    model_builder=create_deep_cnn,\n",
    "    data_handler=data_handler,\n",
    "    model_config=config,\n",
    "    num_runs=10,\n",
    "    epochs_per_run=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Results\n",
    "\n",
    "Now we have two distributions of accuracies. Let's see if there's a statistically significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_accs = list(shallow_results.final_val_accuracies.values())\n",
    "deep_accs = list(deep_results.final_val_accuracies.values())\n",
    "\n",
    "print(\"Shallow CNN:\")\n",
    "print(f\"  Mean: {np.mean(shallow_accs):.4f}\")\n",
    "print(f\"  Std:  {np.std(shallow_accs):.4f}\")\n",
    "print(f\"  Range: [{np.min(shallow_accs):.4f}, {np.max(shallow_accs):.4f}]\")\n",
    "\n",
    "print(\"\\nDeep CNN:\")\n",
    "print(f\"  Mean: {np.mean(deep_accs):.4f}\")\n",
    "print(f\"  Std:  {np.std(deep_accs):.4f}\")\n",
    "print(f\"  Range: [{np.min(deep_accs):.4f}, {np.max(deep_accs):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Test\n",
    "\n",
    "Looking at means isn't enough. We need to know if the difference is statistically significant. Ictonyx uses the Mann-Whitney U test by default, which doesn't assume normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = compare_two_models(\n",
    "    scores_a=shallow_accs,\n",
    "    scores_b=deep_accs,\n",
    "    name_a='Shallow CNN',\n",
    "    name_b='Deep CNN'\n",
    ")\n",
    "\n",
    "print(f\"Test: {test_result.test_name}\")\n",
    "print(f\"p-value: {test_result.p_value:.4f}\")\n",
    "print(f\"\\nConclusion: {test_result.conclusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Comparison\n",
    "\n",
    "A boxplot makes the overlap (or lack thereof) between the two distributions immediately clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the comparison plot\n",
    "comparison_data = {\n",
    "    'raw_data': {\n",
    "        'Shallow CNN': shallow_accs,\n",
    "        'Deep CNN': deep_accs\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_comparison_boxplots(comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We Learned\n",
    "\n",
    "If the p-value is less than 0.05, we can be reasonably confident that the difference is real, not just noise. If it's higher, the models might be performing similarly — and that \"better\" result from the deeper model might just have been a lucky run.\n",
    "\n",
    "This is the difference between:\n",
    "- \"Model B got 98.7% vs Model A's 98.2%\" (anecdote)\n",
    "- \"Model B outperforms Model A with p < 0.01\" (evidence)\n",
    "\n",
    "The second statement is what belongs in a paper, a report, or a production decision.\n",
    "\n",
    "---\n",
    "\n",
    "**Effect size matters too.** Even if the difference is statistically significant, it might be tiny in practical terms. A 0.1% improvement might not be worth the extra complexity. That's a judgment call — but at least now you have the data to make it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
